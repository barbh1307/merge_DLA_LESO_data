{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge DLA LESO Public Information Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files published at the DLA LESO Public Information website do not appear to be cumulative or consistant over time. Therefore, there may be a need to collect the files from the site each quarter. This repository builds a dataset to meet the following needs:   \n",
    " - accumulate the data in a tab-separated format   \n",
    " - merge the two kinds of DLA LESO files into one file   \n",
    " - allow the original data to be pulled out of the merged data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEFORE RUNNING THIS NOTEBOOK, run the following notebooks on the files to be merged:   \n",
    "__Check_DISP_AllStatesAndTerritories.ipynb__ to check *LESO Property Transferred to Participating Agencies*      \n",
    "__Check_DISP_Shipments_Cancellations.ipynb__ to check *LESO Information for Shipments (Tranfers) and Cancellations of Property*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook merges one *LESO Property Transferred to Participating Agencies* file with one *LESO Information for Shipments (Tranfers) and Cancellations of Property* file.  New columns are generated, but the original data is not altered. There should be no problem recreating the original data from the fields in the merged data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guided by the file naming convention used in the [Stanford Open Policing repository](https://github.com/5harad/openpolicing), the merged data is split based on two digit state or territory abbreviation then exported to tab-separated files named \"TWO_LETTER_CODE_leso.tsv\". If the tab-separated files already exist, the notebook will append the merged data to those files. This allows DLA LESO Public Information data from different quarters to be consolidated into one set of files.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merged data has the following fields:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "| Field | Data Type | Description | Original Column | Length | Expected Pattern | null? |   \n",
    "| ----- | ---- | ---- | ---- | ---- |---- | ---- |   \n",
    "||| __Constructed Fields__ |||||   \n",
    "| File | string | file that populated this record | same as LESO filename | varies | see LESOfile variables | no |\n",
    "| Sheet | string | sheet that populated this record | same as sheet name in LESO file | varies | varies | no | \n",
    "| Item_FSG | string | supply category the item belongs to; see [Federal Supply Group Number](https://en.wikipedia.org/wiki/List_of_NATO_Supply_Classification_Groups#References) | file dependent, digits 1&2 of \\['NSN','FSC'\\]| 2 | \\[0-9\\]{2} | no |   \n",
    "| Item_FSC | string | supply class the item belongs to; see [Federal Supply Group Number](https://en.wikipedia.org/wiki/List_of_NATO_Supply_Classification_Groups#References) | file dependent, digits 3&4 of \\['NSN','FSC'\\] | 2 | \\[0-9\\]{2} | no |   \n",
    "| Item_CC | string | country code for where final assembly of item occurred (a.k.a. nation code; see [Federal Supply Group Number](https://en.wikipedia.org/wiki/National_Codification_Bureau) | file dependent, digits 5&6 of \\['NSN'\\] or digits 1&2 of \\['NIIN'\\]| 2 | \\[0-9\\]{2} | no |   \n",
    "| Item_Code | string | supply class the item belongs to; see [Federal Supply Group Number](https://en.wikipedia.org/wiki/List_of_NATO_Supply_Classification_Groups#References) | file dependent, last 7 digits of \\['NSN','NIIN'\\] | 7 | \\[0-9\\]{7} | no |   \n",
    "||| __Fields from Both Files__ |||||   \n",
    "| StateAbbreviation | string | two digit postal abbreviation for U.S. state or territory | State | 2 | \\[A-Z\\]\\[A-Z\\] | no |   \n",
    "| RequestingAgency | string | descriptive name of requesting law enforcement agency | Station Name (LEA) | varies | varies | no |   \n",
    "| ItemDescription | string | descriptive name of requested item | Item Name | varies | varies | no |   \n",
    "| RecordDate | datetime64 | date | file dependent \\['Ship Date','Date Shipped','Date Requested'\\] | 29 | yyyy-mm-ddT00:00:00.000000000 | no |   \n",
    "| AcquisitionValue | float | U.S. dollar amount paid when the item was originally purchased by the government | Acquisition Value | varies | [0-9]+.[0-9]{2} | no |   \n",
    "| Quantity | integer | number of units requested | Quantity | varies | [0-9]+ | no |   \n",
    "| UnitIncrement | string | units of requested item known as unit increments | UI | varies | varies | no |   \n",
    "||| __Fields from All Sheets in AllStatesAndTerritories__ | __fill value 'not in file'__ ||||   \n",
    "| NSN | string | [NATO Stock Number](https://en.wikipedia.org/wiki/NATO_Stock_Number) a government-assigned identifier for requested item | NSN | 9 | \\[0-9\\]{4}-\\[0-9\\]{2}-\\[A-Z0-9\\]{3}-\\[A-Z0-9\\]{4} | no |   \n",
    "| DEMILCode | character | [demilitarization code](https://www.dla.mil/HQ/LogisticsOperations/Services/FIC/DEMILCoding/DEMILCodes/) for level of destruction required when the item leaves Department of Defense control | DEMIL Code | 1 | \\[GPFDCEBQA\\] | no |   \n",
    "| DEMILIC | integer | [demilitarization itegrity code](https://www.dla.mil/HQ/LogisticsOperations/Services/FIC/DEMILCoding/DEMILCodes/) validity of DEMIL Code (a missing value means it has not yet been reviewed), see [FLIS manual](https://www.dla.mil/HQ/LogisticsOperations/TrainingandReference/FLISProcedures/) for more information | DEMIL IC | 1 | [0-9] or blank | yes |   \n",
    "| StationType | string | level of government associated with requesting agency; needs further research | Station Type | 5 | 'State' | no |   \n",
    "||| __Fields from Both Sheets in Shipments_Cancellations__ | __fill value 'not in file'__ ||||   \n",
    "| FSC | string | [Federal Supply Number](https://en.wikipedia.org/wiki/NATO_Stock_Number#Federal_Supply_Classification_Group_(FSCG)) consisting of the Federal Supply Group and Federal Supply Classification | FSC | 4 | \\[0-9\\]{4} | no |   \n",
    "| NIIN | string | [National Item Identification Number](https://en.wikipedia.org/wiki/NATO_Stock_Number#National_Item_Identification_Number_(NIIN)) a Country Code followed by a 7-digit item identifier string | NIIN | 9 | \\[0-9\\]{9} | no |   \n",
    "| Justification | string | descriptive text justifying request; needs further research | Justification | varies | varies | yes |   \n",
    "||| __Fields from Shipments in Shipments_Cancellations__ | __fill value 'not in file'__ ||||   \n",
    "| RequisitionID | string | apparently unique identifier needs further research | Requisition ID | 14 | [A-z0-9]{14} | no |   \n",
    "||| __Fields from Cancellations in Shipments_Cancellations__ | __fill value 'not in file'__ ||||   \n",
    "| CancelledBy | string | apparently agency that cancelled request; needs further research | Cancelled By | varies | varies | yes | \n",
    "| RTDRef | string | apparently unique identifier; needs further research | RTD Ref | 6 or 7 | [0-9]{7} | no |     \n",
    "| ReasonCancelled | string | why request is cancelled; needs further research | Reason Cancelled | varies | varies | yes |   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Libraries used by this notebook.\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#!python --version     #Python 3.8.5\n",
    "# pathlib standard module\n",
    "# sys standard module\n",
    "#pd.__version__       #1.1.2\n",
    "#re.__version__       #2.2.1\n",
    "\n",
    "sys.path.insert(0, \"..\\\\..\\\\scripts\\\\\")\n",
    "from checksumfunctions import get_file_info\n",
    "from checksumfunctions import get_file_hash\n",
    "from notebookfunctions import make_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    VARIABLES THAT CAN BE CUSTOMIZED\n",
    "\n",
    "#    Enter the path to the folder containing all the data files.\n",
    "path_datafiles = \"../../data/\"\n",
    "\n",
    "#    This notebook merges data from a DLA LESO Public Data file\n",
    "#    that has been checked with the following notebook:\n",
    "#         Check_AllStatesAndTerritories.ipynb\n",
    "#    Please run that notebook before setting the 'LESOfile_all' variable.\n",
    "#    \n",
    "#    Enter 'LESO Property Transferred to Participating Agencies' file to be merged.\n",
    "#LESOfile_all = \"DISP_AllStatesAndTerritories_03312020.xlsx\"\n",
    "#LESOfile_all = \"DISP_AllStatesAndTerritories_06302020.xlsx\"\n",
    "#LESOfile_all = \"DISP_AllStatesAndTerritories_09302020.xlsx\"\n",
    "#LESOfile_all = \"DISP_AllStatesAndTerritories_12312020.xlsx\"\n",
    "#LESOfile_all = \"AllStatesAndTerritoriesQTR3FY21.xlsx\"   #period ending 20210630\n",
    "LESOfile_all = \"AllStatesAndTerritoriesQTR4FY21.xlsx\"   #period ending 20210930\n",
    "\n",
    "#    This notebook merges data from a DLA LESO Public Data file\n",
    "#    that has been checked with the following notebook:\n",
    "#         Check_Shipments_Cancellations.ipynb\n",
    "#    Please run that notebook before setting the 'LESOfile_shipcanc' variable.\n",
    "#    \n",
    "#    Enter 'LESO Information for Shipments (Tranfers) and Cancellations of Property'\n",
    "#    file to be merged.\n",
    "#LESOfile_shipcanc = \"DISP_Shipments_Cancellations_01012020_03312020.xlsx\"\n",
    "#LESOfile_shipcanc = \"DISP_Shipments_Cancellations_04012020_06302020.xlsx\"\n",
    "#LESOfile_shipcanc = \"DISP_Shipments_Cancellations_07012020_09302020.xlsx\"\n",
    "#LESOfile_shipcanc = \"DISP_Shipments_Cancellations_10012020_12312020.xlsx\"\n",
    "#LESOfile_shipcanc = \"ShipmentsCancellationsQTR3FY21.xlsx\" #period ending 20210630\n",
    "LESOfile_shipcanc = \"ShipmentsCancellationsQTR4FY21.xlsx\" #period ending 20210930\n",
    "\n",
    "#    Enter the path to where the merged data files will be saved.\n",
    "path_mergedfiles = \"../../data/merged/\"\n",
    "\n",
    "#    The merged data can be split based on a column. The merged data will\n",
    "#    be saved to a series of files based on values in this column.\n",
    "#    By default, it uses the 'StateAbbreviation' column. It can be modified\n",
    "#    a column named 'Quarter' which is generated based on 'RecordDate.'\n",
    "#    If the 'split_by_column' variable is not set, the notebook saves all merged data to one file.\n",
    "split_by_column = 'StateAbbreviation'\n",
    "#split_by_column = 'Quarter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    VARIABLES THAT SHOULD NOT BE CHANGED\n",
    "\n",
    "#    The final, ordered list of columns for the merged data.\n",
    "ordered_columns_list = ['File', 'Sheet', 'StateAbbreviation', 'RequestingAgency',\n",
    "                        'ItemDescription', 'RecordDate', 'AcquisitionValue', 'Quantity',\n",
    "                        'UnitIncrement', 'Item_FSG', 'Item_FSC', 'Item_CC',\n",
    "                        'Item_Code', 'Justification', 'NSN', 'FSC', 'NIIN', 'DEMILCode',\n",
    "                        'DEMILIC', 'StationType', 'RequisitionID' ,'CancelledBy',\n",
    "                        'RTDRef', 'ReasonCancelled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE THE DATA FROM LESOfile_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Expected columns based on columns from Check_AllStatesAndTerritories.ipynb\n",
    "#trans_expected_columns = ['State', 'Station Name (LEA)',\n",
    "#                          'NSN', 'Item Name', 'Quantity', 'UI', 'Acquisition Value',\n",
    "#                          'DEMIL Code', 'DEMIL IC', 'Ship Date', 'Station Type']\n",
    "#20210901add in 20210630, 'Station Name (LEA)' has been changed to 'Agency Name'; all others same\n",
    "trans_expected_columns = ['State', 'Agency Name',\n",
    "                          'NSN', 'Item Name', 'Quantity', 'UI', 'Acquisition Value',\n",
    "                          'DEMIL Code', 'DEMIL IC', 'Ship Date', 'Station Type']\n",
    "#    Dictionary mapping original columns to merged data columns.\n",
    "trans_columns_dictionary = {'State':'StateAbbreviation', 'Agency Name':'RequestingAgency',\n",
    "                            'NSN':'NSN', 'Item Name':'ItemDescription', 'Quantity':'Quantity',\n",
    "                            'UI':'UnitIncrement', 'Acquisition Value':'AcquisitionValue',\n",
    "                            'DEMIL Code':'DEMILCode', 'DEMIL IC':'DEMILIC',\n",
    "                            'Ship Date':'RecordDate', 'Station Type':'StationType'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Read the data from the XLSX file. \n",
    "excel_dict = pd.read_excel(\"file:\" + path_datafiles + LESOfile_all, sheet_name=None)\n",
    "\n",
    "#    Collect information about original data\n",
    "total_transfer_records = sum([len(x) for x in excel_dict.values()])\n",
    "state_to_sheet_dict = {a_df['State'].unique()[0]: sheet for sheet,a_df in excel_dict.items()}\n",
    "\n",
    "#    Create one dataframe with all the data.\n",
    "transfer_df = make_dataframe(excel_dict, 'Ship Date').rename(columns=trans_columns_dictionary)\n",
    "excel_dict.clear()\n",
    "\n",
    "#    Break 'NSN' into NATO Stock Number units.\n",
    "transfer_df = transfer_df.assign(Item_FSG=transfer_df['NSN'].str.replace('-','').str[:2].values,\n",
    "                                 Item_FSC=transfer_df['NSN'].str.replace('-','').str[2:4].values,\n",
    "                                 Item_CC=transfer_df['NSN'].str.replace('-','').str[4:6].values,\n",
    "                                 Item_Code=transfer_df['NSN'].str.replace('-','').str[6:].values,)\n",
    "\n",
    "#    Fill missing columns with 'not in file' value to distinguish them from NaN/null values.\n",
    "transfer_df['FSC'] = 'not in file'\n",
    "transfer_df['NIIN'] = 'not in file'\n",
    "transfer_df['Justification'] = 'not in file'\n",
    "transfer_df['RequisitionID'] = 'not in file'\n",
    "transfer_df['CancelledBy'] = 'not in file'\n",
    "transfer_df['RTDRef'] = 'not in file'\n",
    "transfer_df['ReasonCancelled'] = 'not in file'\n",
    "\n",
    "#    Add 'File' column.\n",
    "transfer_df['File'] = LESOfile_all\n",
    "\n",
    "#    Add 'Sheet' column\n",
    "transfer_df['Sheet'] = [value for key, value in state_to_sheet_dict.items()\n",
    "                        for i in transfer_df['StateAbbreviation'] if i == key]\n",
    "\n",
    "#    Order the columns in preparation for merging.\n",
    "transfer_df = transfer_df[ordered_columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pulled the transfer data from: ')\n",
    "print('%s\\t%s\\t%s' % get_file_info(path_datafiles + LESOfile_all))\n",
    "print('MD5\\t %s' % get_file_hash(path_datafiles + LESOfile_all, 'md5'))\n",
    "print('SHA256\\t %s' % get_file_hash(path_datafiles + LESOfile_all, 'sha256'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total records pulled from %s: %s' % (LESOfile_all, str(total_transfer_records)))\n",
    "print('After prepping the data, the transfer dataframe has %s columns with %s records.'\n",
    "      % (transfer_df.shape[1], transfer_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE THE SHIPMENTS DATA FROM LESOfile_shipcanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Expected columns based on 'SHIPMENTS' columns from Check_Shipments_Cancellations.ipynb\n",
    "current_sheet = 'SHIPMENTS'\n",
    "ship_expected_columns = ['State', 'Station Name (LEA)', 'Requisition ID', 'FSC', 'NIIN',\n",
    "                         'Item Name', 'UI', 'Quantity', 'Acquisition Value', 'Date Shipped',\n",
    "                         'Justification']\n",
    "#    Dictionary mapping original 'SHIPMENTS' columns to merged data columns.\n",
    "ship_columns_dictionary = {'State':'StateAbbreviation', 'Station Name (LEA)':'RequestingAgency',\n",
    "                           'Requisition ID':'RequisitionID', 'FSC':'FSC', 'NIIN':'NIIN',\n",
    "                           'Item Name':'ItemDescription', 'UI':'UnitIncrement', 'Quantity':'Quantity',\n",
    "                           'Acquisition Value':'AcquisitionValue', 'Date Shipped':'RecordDate',\n",
    "                           'Justification':'Justification'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Read the data 'SHIPMENTS' sheet of the XLSX file.\n",
    "shipment_df = make_dataframe(pd.read_excel(\"file:\" + path_datafiles + LESOfile_shipcanc,\n",
    "                                            sheet_name=current_sheet), 'Date Shipped').\\\n",
    "                                 rename(columns=ship_columns_dictionary)\n",
    "\n",
    "#    Collect information about original data\n",
    "total_shipment_records = shipment_df.shape[0]\n",
    "\n",
    "#    Break 'FSC' and 'NIIN' into NATO Stock Number units.\n",
    "shipment_df = shipment_df.assign(Item_FSG=shipment_df['FSC'].astype(str).str[:2],\n",
    "                                 Item_FSC=shipment_df['FSC'].astype(str).str[2:4],\n",
    "                                 Item_CC=shipment_df['NIIN'].str[:2].values,\n",
    "                                 Item_Code=shipment_df['NIIN'].str[2:].values)\n",
    "\n",
    "#    Fill missing columns with 'not in file' value to distinguish them from NaN/null values.\n",
    "shipment_df['NSN'] = 'not in file'\n",
    "shipment_df['DEMILCode'] = 'not in file'\n",
    "shipment_df['DEMILIC'] = 'not in file'\n",
    "shipment_df['StationType'] = 'not in file'\n",
    "shipment_df['CancelledBy'] = 'not in file'\n",
    "shipment_df['RTDRef'] = 'not in file'\n",
    "shipment_df['ReasonCancelled'] = 'not in file'\n",
    "\n",
    "#    Add 'File' column.\n",
    "shipment_df['File'] = LESOfile_shipcanc\n",
    "\n",
    "#    Add 'Sheet' column\n",
    "shipment_df['Sheet'] = current_sheet\n",
    "\n",
    "#    Order the columns in preparation for merging.\n",
    "shipment_df = shipment_df[ordered_columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pulled the shipment data from: ')\n",
    "print('%s\\t%s\\t%s' % get_file_info(path_datafiles + LESOfile_shipcanc))\n",
    "print('MD5\\t %s' % get_file_hash(path_datafiles + LESOfile_shipcanc, 'md5'))\n",
    "print('SHA256\\t %s' % get_file_hash(path_datafiles + LESOfile_shipcanc, 'sha256'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total shipment records pulled from %s: %s' % (LESOfile_shipcanc, str(total_shipment_records)))\n",
    "print('After prepping the data, the shipment dataframe has %s columns with %s records.' \n",
    "      % (shipment_df.shape[1], shipment_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE THE CANCELLATIONS DATA FROM LESOfile_shipcanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Expected columns based on 'CANCELLATIONS' columns from Check_Shipments_Cancellations.ipynb\n",
    "current_sheet = 'CANCELLATIONS'\n",
    "canc_expected_columns = ['Cancelled By', 'RTD Ref', 'State', 'Station Name (LEA)',\n",
    "                         'FSC', 'NIIN', 'Item Name', 'UI', 'Quantity', 'Acquisition Value',\n",
    "                         'Date Requested', 'Justification', 'Reason Cancelled']\n",
    "#    Dictionary mapping original 'CANCELLATIONS' columns to merged data columns.\n",
    "canc_columns_dictionary = {'Cancelled By':'CancelledBy', 'RTD Ref':'RTDRef', \n",
    "                           'State':'StateAbbreviation', 'Station Name (LEA)':'RequestingAgency',\n",
    "                           'FSC':'FSC', 'NIIN':'NIIN', 'Item Name':'ItemDescription',\n",
    "                           'UI':'UnitIncrement', 'Quantity':'Quantity', 'Acquisition Value':'AcquisitionValue',\n",
    "                           'Date Requested':'RecordDate', 'Justification':'Justification',\n",
    "                           'Reason Cancelled':'ReasonCancelled'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Read the data 'CANCELLATIONS' sheet of the XLSX file.\n",
    "cancellation_df = make_dataframe(pd.read_excel(\"file:\" + path_datafiles + LESOfile_shipcanc,\n",
    "                                                sheet_name=current_sheet), 'Date Requested').\\\n",
    "                                     rename(columns=canc_columns_dictionary)\n",
    "\n",
    "#    Collect information about original data\n",
    "total_cancellation_records = cancellation_df.shape[0]\n",
    "\n",
    "#    Break 'FSC' and 'NIIN' into NATO Stock Number units.\n",
    "cancellation_df = cancellation_df.assign(Item_FSG=cancellation_df['FSC'].astype(str).str[:2],\n",
    "                                         Item_FSC=cancellation_df['FSC'].astype(str).str[2:4],\n",
    "                                         Item_CC=cancellation_df['NIIN'].str[:2].values,\n",
    "                                         Item_Code=cancellation_df['NIIN'].str[2:].values)\n",
    "\n",
    "#    Fill missing columns with 'not in file' value to distinguish them from NaN/null values.\n",
    "cancellation_df['NSN'] = 'not in file'\n",
    "cancellation_df['DEMILCode'] = 'not in file'\n",
    "cancellation_df['DEMILIC'] = 'not in file'\n",
    "cancellation_df['StationType'] = 'not in file'\n",
    "cancellation_df['RequisitionID'] = 'not in file'\n",
    "\n",
    "#    Add 'File' column.\n",
    "cancellation_df['File'] = LESOfile_shipcanc\n",
    "\n",
    "#    Add 'Sheet' column\n",
    "cancellation_df['Sheet'] = current_sheet\n",
    "\n",
    "#    Order the columns in preparation for merging.\n",
    "cancellation_df = cancellation_df[ordered_columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pulled the cancellation data from: ')\n",
    "print('%s\\t%s\\t%s' % get_file_info(path_datafiles + LESOfile_shipcanc))\n",
    "print('MD5\\t %s' % get_file_hash(path_datafiles + LESOfile_shipcanc, 'md5'))\n",
    "print('SHA256\\t %s' % get_file_hash(path_datafiles + LESOfile_shipcanc, 'sha256'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total cancellation records pulled from %s: %s' % (LESOfile_shipcanc, str(total_cancellation_records)))\n",
    "print('After prepping the data, the cancellation dataframe has %s columns with %s records.' \n",
    "      % (cancellation_df.shape[1], cancellation_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip military_equipment_distributions_to_law_enforcement_agencies_us.zip in data folder\n",
    "# Merge all dataframes if the columns match.\n",
    "if list(transfer_df.columns) != list(shipment_df.columns):\n",
    "    print('Columns in transfer dataframe do not match columns in shipments dataframe.')\n",
    "elif list(transfer_df.columns) != list(cancellation_df.columns):\n",
    "    print('Columns in transfer dataframe do not match columns in cancellations dataframe.')\n",
    "elif list(shipment_df.columns) != list(cancellation_df.columns):\n",
    "    print('Columns in shipments dataframe do not match columns in cancellations dataframe.')\n",
    "else:\n",
    "    all_data_df = pd.concat([transfer_df, shipment_df, cancellation_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The merged data has', all_data_df.shape[1],\n",
    "      'columns with', all_data_df.shape[0], 'records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Write or append merged data to the TSV file(s).\n",
    "\n",
    "if split_by_column:\n",
    "    if split_by_column == 'Quarter':\n",
    "        all_data_df['Quarter'] = pd.PeriodIndex(all_data_df.RecordDate, freq='Q')\n",
    "        \n",
    "    for i in list(all_data_df[split_by_column].unique()):\n",
    "        my_file = Path(path_mergedfiles + str(i) + '_leso' + '.tsv')\n",
    "        if my_file.exists():\n",
    "            all_data_df[all_data_df[split_by_column] == i].\\\n",
    "                to_csv(my_file, header=False, index=False, mode='a',\n",
    "                       columns=ordered_columns_list, sep='\\t', escapechar=\"\\\\\")\n",
    "        else:\n",
    "            all_data_df[all_data_df[split_by_column] == i].\\\n",
    "                to_csv(my_file, index=False, mode='w',\n",
    "                       columns=ordered_columns_list, sep='\\t', escapechar=\"\\\\\")\n",
    "else:\n",
    "    my_file = Path(path_mergedfiles + 'all_leso' + '.tsv')\n",
    "    if my_file.exists():\n",
    "        all_data_df.to_csv(my_file, header=False, index=False, mode='a',\n",
    "                           columns=ordered_columns_list, sep='\\t', escapechar=\"\\\\\")\n",
    "    else:\n",
    "        all_data_df.to_csv(my_file, index=False, mode='w',\n",
    "                           columns=ordered_columns_list, sep='\\t', escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The merged data file(s) has been saved in the folder', path_mergedfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_file = Path(path_mergedfiles + '/about/README.txt')\n",
    "with open(readme_file, 'a') as a_file:\n",
    "    a_file.write('\\n%s\\t\\t%s' % (get_file_hash(path_datafiles + LESOfile_all, 'md5'), LESOfile_all))\n",
    "    a_file.write('\\n%s\\t\\t%s' % (get_file_hash(path_datafiles + LESOfile_shipcanc, 'md5'), LESOfile_shipcanc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
