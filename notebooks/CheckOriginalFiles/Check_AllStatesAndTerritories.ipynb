{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check LESO Transferred Property File (DISP_AllStatesAndTerritories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook checks that the file containing transferred inventory data matches the structure of previous versions of the file. The following data files are used in this notebook:   \n",
    " - A CSV file containing state/territory names followed by their postal abbreviations. Both U.S. states and territories are required. \n",
    "   - This file can be populated with data from [US Postal Service Publication 28](https://pe.usps.com/text/pub28/28apb.htm).\n",
    "   - The postal_file variable in this notebook should be set to the name of this file.\n",
    " - An Excel file containing transferred inventory data up to the quarter specified in the file name (for example: DISP_AllStatesAndTerritories_03312020.xlsx).   \n",
    "   - To download the file expected by this notebook, click on 'ALASKA - WYOMING AND US TERRITORIES' from the *LESO Property Transferred to Participating Agencies* section of the [DLA LESO Public Information](https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/PublicInformation/) website.   \n",
    "   - The LESO_file variable in this notebook should be set to the name of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook expects the Excel file to have one sheet for each state or territory with agencies that received property through the program. Each sheet has the following fields:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "| Field | Data Type | Description | Length | Expected Pattern | null? |   \n",
    "| ----- | ---- | ---- | ---- | ---- |---- |   \n",
    "| State | string | two digit postal abbreviation for U.S. state or territory | 2 | \\[A-Z\\]\\[A-Z\\] | no |   \n",
    "| Station Name (LEA) | string | descriptive name of requesting law enforcement agency | varies | varies | no |   \n",
    "| NSN | string | [NATO Stock Number](https://en.wikipedia.org/wiki/NATO_Stock_Number) a government-assigned identifier for requested item | 9 | \\[0-9\\]{4}-\\[0-9\\]{2}-\\[A-Z0-9\\]{3}-\\[A-Z0-9\\]{4} | no |   \n",
    "| Item Name | string | descriptive name of requested item | varies | varies | no |   \n",
    "| UI | string | units of requested item known as unit increments | varies | varies | no |   \n",
    "| Quantity | integer | number of units requested | varies | [0-9]+ | no |   \n",
    "| Acquisition Value | float | U.S. dollar amount paid when the item was originally purchased by the government | varies | [0-9]+.[0-9]{2} | no |   \n",
    "| DEMIL Code | character | [demilitarization code](https://www.dla.mil/HQ/LogisticsOperations/Services/FIC/DEMILCoding/DEMILCodes/) for level of destruction required when the item leaves Department of Defense control | 1 | \\[GPFDCEBQA\\] | no |   \n",
    "| DEMIL IC | integer | [demilitarization itegrity code](https://www.dla.mil/HQ/LogisticsOperations/Services/FIC/DEMILCoding/DEMILCodes/) validity of DEMIL Code (a missing value means it has not yet been reviewed), see [FLIS manual](https://www.dla.mil/HQ/LogisticsOperations/TrainingandReference/FLISProcedures/) for more information | 1 | [0-9] or blank | yes |   \n",
    "| Ship Date | datetime64 | date transfered; needs further research | 29 | yyyy-mm-ddT00:00:00.000000000 | no |   \n",
    "| Station Type | string | level of government associated with requesting agency; needs further research | 5 | 'State' | no |   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Libraries used by this notebook.\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "\n",
    "#!python --version  #Python 3.8.5\n",
    "# sys is a standard library\n",
    "#pd.__version__     #1.1.2 \n",
    "#re.__version__     #2.2.1\n",
    "\n",
    "sys.path.insert(0, \"..\\\\..\\\\scripts\\\\\")\n",
    "from checksumfunctions import get_file_info\n",
    "from checksumfunctions import get_file_hash\n",
    "from notebookfunctions import get_unique_values\n",
    "from notebookfunctions import get_unexpected_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    VARIABLES THAT CAN BE CUSTOMIZED\n",
    "\n",
    "#    Enter the path to the folder containing all the data files.\n",
    "path_datafiles = \"../../data/\"\n",
    "\n",
    "#    This notebook expects a comma-separated file consisting of:\n",
    "#        full name,postal abbreviation\n",
    "#    The values can be downloaded from U.S. Postal Service Publication 28:\n",
    "#        https://pe.usps.com/text/pub28/28apb.htm\n",
    "#    \n",
    "#    Enter the name of the file containing postal codes.\n",
    "postal_file = 'USPS_StateAbbreviations.csv'\n",
    "\n",
    "#    Get the 'LESO Property Transferred to Participating Agencies' file from \n",
    "#        Defense Logicstics Agency Law Enforcement Support Office Public Information\n",
    "#    The original name of the data file should be in the form:\n",
    "#        DISP_AllStatesAndTerritories_mmddyyyy.xlsx  \n",
    "#    \n",
    "#    Enter the name of the LESO file to be checked.\n",
    "#LESO_file = \"DISP_AllStatesAndTerritories_03312020.xlsx\"\n",
    "#LESO_file = \"DISP_AllStatesAndTerritories_06302020.xlsx\"\n",
    "LESO_file = \"DISP_AllStatesAndTerritories_09302020.xlsx\"\n",
    "#LESO_file = \"DISP_AllStatesAndTerritories_12312020.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    VARIABLES THAT SHOULD NOT BE CHANGED\n",
    "\n",
    "#    Assume the file is good to merge.\n",
    "flag_file_good_to_merge = True\n",
    "\n",
    "#    Expected columns based on columns from previous files.\n",
    "expected_columns = ['State', 'Station Name (LEA)',\n",
    "                    'NSN', 'Item Name', 'Quantity', 'UI', 'Acquisition Value',\n",
    "                    'DEMIL Code', 'DEMIL IC', 'Ship Date', 'Station Type']\n",
    "\n",
    "#    Expected 'Station Types' based on values from previous files.\n",
    "expected_station_types = ['State']\n",
    "\n",
    "#    Expected 'DEMIL Codes' based on DOD 4160.28 DEMIL Program or\n",
    "#    DOD 4100.39M FLIS Manual at this website:\n",
    "#        https://www.dla.mil/HQ/LogisticsOperations/Services/FIC/DEMILCoding/DEMILCodes/\n",
    "expected_demil_codes = ['G', 'P', 'F', 'D', 'C', 'E', 'B', 'Q', 'A']\n",
    "\n",
    "\n",
    "#    Expected 'DEMIL IC' values based on DOD 4160.28 DEMIL Program or\n",
    "#    DOD 4100.39M FLIS Manual at this website:\n",
    "#        https://www.dla.mil/HQ/LogisticsOperations/Services/FIC/DEMILCoding/DEMILCodes/\n",
    "expected_demil_integritycodes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "#    Build a dictionary of expected postal abbreviations based on the file\n",
    "#    named by the 'postal_file' variable.\n",
    "#        key: state abbreviation\n",
    "#        value: state name\n",
    "expected_postal_abbreviations = pd.read_csv(path_datafiles + postal_file, header=None,\n",
    "                                            quotechar = \"'\").set_index([1])[0].to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Read the data from the XLSX file.\n",
    "transfer_dict = pd.read_excel(\"file:\" + path_datafiles + LESO_file, sheet_name=None)\n",
    "#    transfer_dict is a dictionary of all sheets in the LESO_file\n",
    "#         keys are full state/territory names\n",
    "#         values are a single dataframe of all transfers for that state/territory\n",
    "#    The records may be cumulative up to this quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This notebook is checking: ')\n",
    "print('%s\\t%s\\t%s' % get_file_info(path_datafiles + LESO_file))\n",
    "print('MD5\\t %s' % get_file_hash(path_datafiles + LESO_file, 'md5'))\n",
    "print('SHA256\\t %s' % get_file_hash(path_datafiles + LESO_file, 'sha256'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THESE QUESTIONS DECIDE IF THIS FILE CAN BE MERGED WITH FILES FROM PREVIOUS QUARTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### QUESTION A: Are the values of 'State' valid U.S. postal abbreviations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_postal_abbreviations = get_unexpected_values(set(get_unique_values(transfer_dict,'State')),\n",
    "                                                        set(expected_postal_abbreviations.keys()))\n",
    "if len(unexpected_postal_abbreviations) == 0:\n",
    "    print('Only valid state and territory abbreviations were found.')\n",
    "else:\n",
    "    print('These state or territory abbreviations are not valid:\\n', list(unexpected_postal_abbreviations))\n",
    "    flag_file_good_to_merge = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### QUESTION B: Does each sheet have exactly one value for 'State'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inconsistant_postal_abbreviations = [state_name for state_name in transfer_dict\n",
    "                                     if len(transfer_dict[state_name]['State'].unique()) != 1]\n",
    "if len(inconsistant_postal_abbreviations) == 0:\n",
    "    print('All sheets have exactly one state/territory abbreviation.')\n",
    "else:\n",
    "    print('These states do not have exactly one state/territory abbreviation:\\n', inconsistant_postal_abbreviations)\n",
    "    flag_file_good_to_merge = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### QUESTION C: Do all sheets have the expected columns? (All sheets should have the same columns.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_discrepancy = [state_name for state_name in transfer_dict\n",
    "                      if (expected_columns != transfer_dict[state_name].columns.tolist())]\n",
    "if len(column_discrepancy) == 0:\n",
    "    print('Only expected columns were found.')\n",
    "else:\n",
    "    print('Columns need to be checked on these states:\\n',column_discrepancy)\n",
    "    flag_file_good_to_merge = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### QUESTION D: Can this file be merged with DLA LESO Public Data files from previous quarters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_file_good_to_merge:\n",
    "    print('Yes, this file can be merged with DLA LESO Public Data files from previous quarters.')\n",
    "else:\n",
    "    print('No, this file cannot be merged for the following reasons:')\n",
    "    if len(unexpected_postal_abbreviations) > 0:\n",
    "        print('See Question A')\n",
    "    if len(inconsistant_postal_abbreviations) > 0:\n",
    "        print('See Question B')\n",
    "    if len(column_discrepancy) > 0:\n",
    "        print('See Question C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL INFORMATION ABOUT THE ORIGINAL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### QUESTION 1: What is the basic shape of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Transfers file has', len(transfer_dict), 'states/territories.')\n",
    "print('Transfers file has', sum([len(x) for x in transfer_dict.values()]), 'rows across all states/territories.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### QUESTION 2: Do the state or territory names on all sheets match U.S. postal names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_state_names = [state_name for state_name in transfer_dict \n",
    "                         if state_name not in expected_postal_abbreviations.values()]\n",
    "if len(incorrect_state_names) == 0:\n",
    "    print('All state/territory names match U.S. Postal Service names.')\n",
    "else:\n",
    "    for i in incorrect_state_names:\n",
    "        abbreviations = list(transfer_dict[i]['State'].unique())\n",
    "        print('Misspelled state/territory name : ', i,' abbreviated as ',abbreviations)\n",
    "        if flag_file_good_to_merge:\n",
    "            print('\\tBest guess state/territory name: ', \n",
    "                  expected_postal_abbreviations[abbreviations[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### QUESTION 3: How many total null/NaN values in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = pd.DataFrame(columns=expected_columns)\n",
    "count = 0\n",
    "for state_name in transfer_dict:\n",
    "    for k,v in transfer_dict[state_name].isna().sum().iteritems():\n",
    "        null_counts.loc[count, k] = v\n",
    "    null_counts.loc[count, 'State Name'] = state_name\n",
    "    count+=1\n",
    "for col,num_null in null_counts[expected_columns].sum().astype(int).items():\n",
    "    if num_null > 0:\n",
    "        print('Found', num_null, 'null values in', col, 'across all states/territories.')\n",
    "print('All other columns had no null values across all states/territories.')\n",
    "#    Uncomment the following if to see null values by state.\n",
    "#null_counts.set_index('State Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### QUESTION 4: Are the unique values of 'Station Type' as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_station_types = get_unexpected_values(set(get_unique_values(transfer_dict,'Station Type')),\n",
    "                                                 set(expected_station_types))\n",
    "if len(unexpected_station_types) == 0:\n",
    "    print('\\nOnly expected station types found.')\n",
    "else:\n",
    "    print('\\nFound these unexpected station types:',list(unexpected_station_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### QUESTION 5: Are the unique values of 'DEMIL Code' as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_demil_codes = get_unexpected_values(set(get_unique_values(transfer_dict,'DEMIL Code')),\n",
    "                                               set(expected_demil_codes))\n",
    "if len(unexpected_demil_codes) == 0:\n",
    "    print('\\nOnly expected DEMIL codes found.')\n",
    "else:\n",
    "    print('\\nFound these unexpected DEMIL codes:',list(unexpected_demil_codes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### QUESTION 6: Are the unique values of 'DEMIL IC' as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_demil_integritycodes = get_unexpected_values(set(get_unique_values(transfer_dict,'DEMIL IC')),\n",
    "                                                        set(expected_demil_integritycodes))\n",
    "non_nan_list = []\n",
    "[non_nan_list.append(ic) for ic in unexpected_demil_integritycodes if pd.notna(ic)]\n",
    "if len(non_nan_list) > 0:\n",
    "    print('Found these unexpected DEMIL integrity codes:',non_nan_list)\n",
    "else:\n",
    "    print('Only expected integrity codes found.')\n",
    "print('Found',len(unexpected_demil_integritycodes) - len(non_nan_list),\n",
    "      'states with NaN DEMIL integrity codes values.\\nRecall a missing DEMIL integrity codes means the DEMIL code has not yet been reviewed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### QUESTION 7: How many unique values are in each column of each sheet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = pd.DataFrame(columns=expected_columns)\n",
    "count = 0\n",
    "for state_name in transfer_dict:\n",
    "    for col, num_uniq in transfer_dict[state_name].nunique().iteritems():\n",
    "        unique_counts.loc[count, col] = num_uniq\n",
    "    unique_counts.loc[count, 'State Name'] = state_name\n",
    "    count += 1\n",
    "unique_counts.set_index('State Name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
